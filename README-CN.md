# mcp-smart-fetch

[![Rust](https://img.shields.io/badge/rust-1.75%2B-blue.svg)](https://rustlang.org)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![MCP](https://img.shields.io/badge/MCP-2024--11--05-orange.svg)](https://modelcontextprotocol.io)

åŸºäº Rust å®˜æ–¹ MCP SDK æ„å»ºçš„æ™ºèƒ½æ–‡æ¡£å†…å®¹æå–æœåŠ¡ï¼Œæ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼ï¼Œé›†æˆäº† LLM API è¿›è¡Œæ™ºèƒ½å†…å®¹æå–ï¼Œå¯ä½œä¸ºæ ‡å‡† MCP æœåŠ¡å™¨è¿è¡Œã€‚

## âœ¨ ç‰¹æ€§

- ğŸš€ **é«˜æ€§èƒ½å¼‚æ­¥æ¶æ„** - åŸºäº Tokio å¼‚æ­¥è¿è¡Œæ—¶
- ğŸ§  **æ™ºèƒ½å†…å®¹æå–** - é›†æˆå¤šç§ LLM API
- ğŸ“„ **å¤šæ ¼å¼æ”¯æŒ** - TXT, MD, JSON, YAML, TOML, XML, CSV
- ğŸ”§ **MCP æœåŠ¡å™¨** - æ ‡å‡† Model Context Protocol æœåŠ¡å™¨
- âš™ï¸ **çµæ´»é…ç½®** - æ”¯æŒé…ç½®æ–‡ä»¶å’Œç¯å¢ƒå˜é‡
- ğŸ³ **å®¹å™¨åŒ–æ”¯æŒ** - Docker éƒ¨ç½²å°±ç»ª
- ğŸ§ª **å®Œæ•´æµ‹è¯•** - å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•è¦†ç›–

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Rust 1.75+
- LLM API å¯†é’¥ï¼ˆOpenAIã€Claudeã€é˜¿é‡Œäº‘ç­‰ï¼‰

### å®‰è£…

```bash
git clone https://github.com/yourusername/mcp-smart-fetch.git
cd mcp-smart-fetch
cargo build --release
```

### é…ç½®

1. å¤åˆ¶ç¯å¢ƒå˜é‡ç¤ºä¾‹ï¼š
```bash
cp .env.example .env
```

2. ç¼–è¾‘ `.env` æ–‡ä»¶ï¼Œå¡«å…¥ä½ çš„ API å¯†é’¥ï¼š
```bash
LLM_API_KEY="your-api-key-here"
LLM_MODEL="gpt-4"
LLM_API_ENDPOINT="https://api.openai.com/v1/chat/completions"
```

### åŸºæœ¬ä½¿ç”¨

#### ä»æ–‡ä»¶æå–å†…å®¹

```bash
cargo run -- extract input.txt
cargo run -- extract --input document.pdf --prompt "æ€»ç»“æ–‡æ¡£è¦ç‚¹"
cargo run -- extract -i data.json -o result.txt
```

#### ä»æ–‡æœ¬æå–å†…å®¹

```bash
cargo run -- extract-text --text "è¿™æ˜¯ä¸€æ®µéœ€è¦åˆ†æçš„æ–‡æœ¬..."
cargo run -- extract-text -t "æ–‡æœ¬å†…å®¹" -p "æå–å…³é”®ä¿¡æ¯"
```

#### å¯åŠ¨ MCP æœåŠ¡å™¨

```bash
# å¯åŠ¨ MCP æœåŠ¡å™¨ï¼ˆstdio æ¨¡å¼ï¼‰
cargo run -- serve

# æŸ¥çœ‹è¯¦ç»†é…ç½®ä¿¡æ¯
cargo run --verbose serve
```

## ğŸ”§ MCP æœåŠ¡å™¨

mcp-smart-fetch å¯ä½œä¸ºæ ‡å‡† MCP æœåŠ¡å™¨è¿è¡Œï¼Œæä¾›ä»¥ä¸‹å·¥å…·ï¼š

### å¯ç”¨å·¥å…·

1. **extract_from_file** - ä»æ–‡ä»¶æå–æ™ºèƒ½å†…å®¹
2. **extract_from_text** - ä»æ–‡æœ¬æå–æ™ºèƒ½å†…å®¹
3. **get_config** - è·å–æœåŠ¡å™¨é…ç½®ä¿¡æ¯
4. **list_supported_formats** - åˆ—å‡ºæ”¯æŒçš„æ–‡æ¡£æ ¼å¼

### å®¢æˆ·ç«¯é…ç½®

#### Claude Desktop

åœ¨ `claude_desktop_config.json` ä¸­æ·»åŠ ï¼š

```json
{
  "mcpServers": {
    "smart-fetch": {
      "command": "cargo",
      "args": ["run", "--", "serve"],
      "env": {
        "LLM_API_KEY": "your-api-key"
      }
    }
  }
}
```

#### Docker éƒ¨ç½²

```bash
# ä½¿ç”¨ç¯å¢ƒå˜é‡
docker run --env-file .env -v $(pwd)/templates:/app/templates mcp-smart-fetch serve

# ä½¿ç”¨ docker-compose
docker-compose up mcp-server
```

## âš™ï¸ é…ç½®

### ç¯å¢ƒå˜é‡

é¡¹ç›®æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡è¿›è¡Œå®Œæ•´é…ç½®ï¼Œç¯å¢ƒå˜é‡ä¼˜å…ˆçº§é«˜äºé…ç½®æ–‡ä»¶ã€‚

#### LLM é…ç½®
- `LLM_API_KEY` - LLM API å¯†é’¥ï¼ˆå¿…éœ€ï¼‰
- `LLM_API_ENDPOINT` - API ç«¯ç‚¹ URL
- `LLM_MODEL` - ä½¿ç”¨çš„æ¨¡å‹åç§°
- `LLM_MAX_TOKENS` - æœ€å¤§ token æ•° (u32)
- `LLM_TEMPERATURE` - æ¸©åº¦å‚æ•° (f64, 0.0-2.0)
- `LLM_TIMEOUT_SECONDS` - è¯·æ±‚è¶…æ—¶æ—¶é—´ (u64, ç§’)

#### æœåŠ¡å™¨é…ç½®
- `SERVER_HOST` - æœåŠ¡å™¨ç›‘å¬åœ°å€
- `SERVER_PORT` - æœåŠ¡å™¨ç«¯å£ (u16)
- `SERVER_MAX_CONNECTIONS` - æœ€å¤§è¿æ¥æ•° (u32)
- `SERVER_REQUEST_TIMEOUT_SECONDS` - è¯·æ±‚è¶…æ—¶æ—¶é—´ (u64, ç§’)

#### å¤„ç†é…ç½®
- `TEMPLATES_DIR` - æ¨¡æ¿ç›®å½•è·¯å¾„
- `DEFAULT_TEMPLATE` - é»˜è®¤æ¨¡æ¿åç§°
- `MAX_DOCUMENT_SIZE_MB` - æœ€å¤§æ–‡æ¡£å¤§å° (f64, MB)
- `CHUNK_SIZE` - åˆ†å—å¤§å° (usize)
- `ENABLE_PREPROCESSING` - æ˜¯å¦å¯ç”¨é¢„å¤„ç† (bool)

#### æ¸…ç†é…ç½®
- `ENABLE_CLEANING` - æ˜¯å¦å¯ç”¨æ¸…ç†åŠŸèƒ½ (bool)
- `REMOVE_BASE64_IMAGES` - æ˜¯å¦ç§»é™¤ base64 å›¾ç‰‡ (bool)
- `REMOVE_BINARY_DATA` - æ˜¯å¦ç§»é™¤äºŒè¿›åˆ¶æ•°æ® (bool)
- `REMOVE_HTML_TAGS` - æ˜¯å¦ç§»é™¤ HTML æ ‡ç­¾ (bool)
- `NORMALIZE_WHITESPACE` - æ˜¯å¦è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦ (bool)
- `MAX_STRING_LENGTH` - æœ€å¤§å­—ç¬¦ä¸²é•¿åº¦ (usize)

### é…ç½®æ–‡ä»¶

é…ç½®æ–‡ä»¶ä½äº `config/config.toml`ï¼Œæ”¯æŒåˆ†å±‚é…ç½®ï¼š

```toml
[llm]
api_endpoint = "https://api.openai.com/v1/chat/completions"
model = "gpt-4"
max_tokens = 32768
temperature = 0.7

[server]
host = "127.0.0.1"
port = 8080

[processing]
max_document_size_mb = 10.0
chunk_size = 4000
supported_formats = ["txt", "md", "json", "yaml", "yml", "toml", "xml", "csv"]
```

### æŸ¥çœ‹é…ç½®ä¿¡æ¯

```bash
# æŸ¥çœ‹æ‰€æœ‰æ”¯æŒçš„ç¯å¢ƒå˜é‡
cargo run -- env-vars

# æŸ¥çœ‹è¯¦ç»†é…ç½®ä¿¡æ¯
cargo run --verbose extract-text --text "test"
```

## ğŸ§ª æµ‹è¯•

### è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
cargo test

# è¿è¡Œå•å…ƒæµ‹è¯•
cargo test --lib

# è¿è¡Œé›†æˆæµ‹è¯•ï¼ˆMCP æœåŠ¡å™¨ï¼‰
cargo test --test mcp_server_test

# è¿è¡Œç‰¹å®šæµ‹è¯•
cargo test test_extract_from_text
```

### æµ‹è¯•è¦†ç›–

- å•å…ƒæµ‹è¯•ï¼šå„ä¸ªæ¨¡å—çš„ç‹¬ç«‹åŠŸèƒ½
- MCP æœåŠ¡å™¨æµ‹è¯•ï¼šå®Œæ•´çš„ MCP åè®®æµ‹è¯•
- é›†æˆæµ‹è¯•ï¼šç«¯åˆ°ç«¯çš„å†…å®¹æå–æµç¨‹

## ğŸ³ Docker éƒ¨ç½²

### æ„å»ºé•œåƒ

```bash
docker build -t mcp-smart-fetch .
```

### è¿è¡Œå®¹å™¨

```bash
# ä½¿ç”¨ç¯å¢ƒå˜é‡
docker run --env-file .env -v $(pwd)/templates:/app/templates mcp-smart-fetch

# ä½œä¸º MCP æœåŠ¡å™¨è¿è¡Œ
docker run --env-file .env mcp-smart-fetch serve
```

### Docker Compose

```yaml
version: '3.8'
services:
  mcp-server:
    build: .
    command: ["serve"]
    environment:
      - LLM_API_KEY=${LLM_API_KEY}
      - LLM_MODEL=${LLM_MODEL}
    volumes:
      - ./templates:/app/templates
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
mcp-smart-fetch/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.rs              # ä¸»ç¨‹åºå…¥å£
â”‚   â”œâ”€â”€ lib.rs               # åº“å…¥å£
â”‚   â”œâ”€â”€ config.rs            # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ mcp_server.rs        # MCP æœåŠ¡å™¨å®ç°
â”‚   â”œâ”€â”€ llm_client.rs        # LLM å®¢æˆ·ç«¯
â”‚   â”œâ”€â”€ document.rs          # æ–‡æ¡£å¤„ç†
â”‚   â”œâ”€â”€ prompt_template.rs   # æç¤ºè¯æ¨¡æ¿
â”‚   â”œâ”€â”€ cleaner.rs           # å†…å®¹æ¸…ç†
â”‚   â”œâ”€â”€ progress.rs          # è¿›åº¦æ˜¾ç¤º
â”‚   â””â”€â”€ error.rs             # é”™è¯¯å¤„ç†
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit_test.rs         # å•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ integration_test.rs  # é›†æˆæµ‹è¯•
â”‚   â”œâ”€â”€ cleaning_test.rs     # æ¸…ç†æµ‹è¯•
â”‚   â””â”€â”€ mcp_server_test.rs   # MCP æœåŠ¡å™¨æµ‹è¯•
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.toml          # é…ç½®æ–‡ä»¶
â”œâ”€â”€ templates/               # æ¨¡æ¿ç›®å½•
â”œâ”€â”€ examples/                # ç¤ºä¾‹æ–‡ä»¶
â”œâ”€â”€ .env.example            # ç¯å¢ƒå˜é‡ç¤ºä¾‹
â”œâ”€â”€ docker-compose.yml       # Docker Compose é…ç½®
â”œâ”€â”€ Dockerfile              # Docker é•œåƒé…ç½®
â””â”€â”€ README.md               # é¡¹ç›®æ–‡æ¡£
```

## ğŸ”§ å¼€å‘

### å¼€å‘ç¯å¢ƒè®¾ç½®

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/yourusername/mcp-smart-fetch.git
cd mcp-smart-fetch

# å®‰è£… Rust å·¥å…·é“¾
rustup install stable
rustup component add clippy rustfmt

# å®‰è£…é¢„æäº¤é’©å­ï¼ˆå¯é€‰ï¼‰
cargo install pre-commit
pre-commit install
```

### å¼€å‘å‘½ä»¤

```bash
# æ„å»ºé¡¹ç›®
cargo build

# è¿è¡Œå¼€å‘ç‰ˆæœ¬
cargo run

# è¿è¡Œæµ‹è¯•
cargo test

# ä»£ç æ ¼å¼åŒ–
cargo fmt

# ä»£ç æ£€æŸ¥
cargo clippy

# ç”Ÿæˆæ–‡æ¡£
cargo doc --open
```

### æ·»åŠ æ–°çš„ LLM æä¾›å•†

1. åœ¨ `src/llm_client.rs` ä¸­æ·»åŠ æ–°çš„è¯·æ±‚æ ¼å¼
2. åœ¨ `config/config.toml` ä¸­æ·»åŠ æ–°çš„é…ç½®ç¤ºä¾‹
3. æ›´æ–°æ–‡æ¡£ä¸­çš„ç¯å¢ƒå˜é‡è¯´æ˜
4. æ·»åŠ ç›¸åº”çš„æµ‹è¯•ç”¨ä¾‹

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®ä»£ç ï¼è¯·éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. æ‰“å¼€ Pull Request

## ğŸ“ æ”¯æŒ

- ğŸ“§ é‚®ä»¶ï¼šyour-email@example.com
- ğŸ› é—®é¢˜åé¦ˆï¼š[GitHub Issues](https://github.com/yourusername/mcp-smart-fetch/issues)
- ğŸ“– æ–‡æ¡£ï¼š[Wiki](https://github.com/yourusername/mcp-smart-fetch/wiki)

## ğŸ™ è‡´è°¢

- [Model Context Protocol](https://modelcontextprotocol.io/) - MCP åè®®
- [rmcp](https://github.com/modelcontextprotocol/rust-sdk) - Rust MCP SDK
- [Tokio](https://tokio.rs/) - å¼‚æ­¥è¿è¡Œæ—¶
- [Handlebars](https://handlebarsjs.com/) - æ¨¡æ¿å¼•æ“

---

â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™å®ƒä¸€ä¸ª starï¼